
# Version 0.3 -10192023

# Multi-Master K8S Cluster Configuration

# VM OS used = Ubuntu 22.04 for all nodes
# K8S Cluster Version = 1.28.2 (available in Oct 2023)

# VM Nodes or Server IP range used = 192.168.100.0/24 (256 static ips - 254 useable)
# K8S Internal IP Range taken = 10.244.0.0/16

# Total controllers considered in this project
# Minimun CPU/Memory required are 2 CPU / 2 Gib
# I have took 2 CPU / 4 Gib for all contol plane nodes
# controller0.13infotech.com - controllerplane - 01
# controller1.13infotech.com - controllerplane - 02
# controller2.13infotech.com - controllerplane - 03

# Total Worker Nodes considered in this project
# Minimun CPU/Memory required are 1 CPU / 1 Gib
# I have took 2 CPU / 4 Gib for all woker nodes
# wn01.13infotech.com - Worker Node 01
# wn02.13infotech.com - Worker Node 02
# wn03.13infotech.com - Worker Node 03

# I have took 2 CPU / 4 Gib for Load Balancer Control Node
# controller3.13infotech.com - Load Balancer Node hosting haproxy - 01
# This Load Balance Node will route all control planes upon intitalize.

# Firewall Rules (Ingress)
# Master Node: 2379,6443,10250,10251,10252 
# Worker Node: 10250,30000-32767

# Common commands need to know for all cluster nodes

# To Gain the root access on Ubuntu 22.x
sudo -i

# NOT Mandatory. For better visibility.
# Add below lines to ~/.bashrc
# Master Node:
PS1="\e[0;33m[\u@\h \W]\$ \e[m "

# Worker Node:
PS1="\e[0;36m[\u@\h \W]\$ \e[m "

# To update and upgarde Ubuntu 22.x Packages before k8s cluster installation
sudo apt update && sudo apt upgrade -y

# To Check the DNS Status on Ubuntu 22.x
resolvectl status

# To edit the DNS Servers on Ubuntu 22.x
sudo vi /etc/netplan/00-installer-config.yaml

#To Apply the DNS Changes
sudo netplan apply

# update all nodes to check if the OS is already updated and can download system updates from Internet
sudo apt update
sudo apt upgrade

# First set the hostname of all the nodes
# I own the domain 13infotech.com thats why I have used here, you can use any domain that you own

sudo hostnamectl set-hostname "controller0.13infotech.com"
sudo hostnamectl set-hostname "controller1.13infotech.com"
sudo hostnamectl set-hostname "controller2.13infotech.com"
sudo hostnamectl set-hostname "controller3.13infotech.com"
sudo hostnamectl set-hostname "wn01.13infotech.com"
sudo hostnamectl set-hostname "wn02.13infotech.com"
sudo hostnamectl set-hostname "wn03.13infotech.com"

# Optional Step
exec bash

# Sample for /etc/hosts on all nodes
# You must copy all these enteries on all nodes - control planes,HA Proxy Node,Woker Nodes,Any Laptop or Jump Server using to access all these nodes.

sudo vi /etc/host

192.168.100.101 controller0 controller0.13infotech.com
192.168.100.100 controller1 controller1.13infotech.com
192.168.100.99 controller2 controller2.13infotech.com
192.168.100.95 controller3 controller3.13infotech.com
192.168.100.98 wn01 wn01.13infotech.com
192.168.100.97 wn02 wn02.13infotech.com
192.168.100.96 wn02 wn02.13infotech.com

# For our cluster, we will use HAPROXY as the primary loadbalancer
# Install haproxy
sudo apt-get install haproxy -y
# Edit haproxy configuration
vi /etc/haproxy/haproxy.cfg
# Add the below lines to create a frontend configuration for the loadbalancer
frontend fe-apiserver
   bind 0.0.0.0:6443
   mode tcp
   option tcplog
   default_backend be-apiserver
# Add the below lines to create a backend configuration for master1,master2 and master3 nodes at port 6443
# Note : 6443 is the default port of kube-apiserver
backend be-apiserver
   mode tcp
   option tcplog
   option tcp-check
   balance roundrobin
   default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100

       server master1 controller0.13infotech.com:6443 check
       server master2 controller1.13infotech.com:6443 check
       server master3 controller2.13infotech.com:6443 check
# to check the haproxy configuration errors
haproxy -c -f /etc/haproxy/haproxy.cfg

# Restart and verify the haproxy
sudo systemctl restart haproxy
sudo systemctl status haproxy
sudo systemctl enable haproxy

# Ensure haproxy is in running status
# Run nc command as below
nc -v localhost 6443

# Output will be like this
# Connection to localhost 6443 port [tcp/*] succeeded!



# Mandatory steps to turn off the swap on all nodes excluding controller3.13infotech.com
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

# Mandatory step to turn off the swap manually else upon restart of any node, swap will be active again
sudo vim /etc/fstab
#/swap.img   #add # infront of /swap.img to make it permanently disabled
# Restart the node after making this. 

# once done all steps are completed on all nodes, please reboot all nodes
sudo reboot

# After restart check again to see if swap is disabled by executing these commands

sudo mount -a
free -h


# Mandatory steps to turn off the swap on all nodes excluding controller3.13infotech.com

# Load the following kernel modules on all the nodes
sudo tee /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# Set the following Kernel parameters for Kubernetes
sudo tee /etc/sysctl.d/kubernetes.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

# Reload the above changes, run
sudo sysctl --system

# Install the Containerd Runtime
sudo apt install -y curl gnupg2 software-properties-common apt-transport-https ca-certificates
# Enable the docker repository
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmour -o /etc/apt/trusted.gpg.d/docker.gpg
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
# Now, run following apt command to install containerd
sudo apt update
sudo apt install -y containerd.io

# Configure containerd so that it starts using systemd as cgroup.
containerd config default | sudo tee /etc/containerd/config.toml >/dev/null 2>&1
sudo sed -i 's/SystemdCgroup \= false/SystemdCgroup \= true/g' /etc/containerd/config.toml
# Restart and enable containerd service
sudo systemctl restart containerd
sudo systemctl enable containerd

# Add Apt Repository for Kubernetes - Google deprecating repositories
sudo apt-get install -y apt-transport-https ca-certificates curl
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmour -o /etc/apt/trusted.gpg.d/kubernetes-xenial.gpg
sudo apt-add-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"


# Kubernetes package repositories
# These instructions are for Kubernetes 1.28.x
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list


# Install Kubectl, Kubeadm and Kubelet
sudo apt update
# Optional Commands - Version Specific
# apt-cache madison kubeadm
# sudo apt install -y kubelet=1.24.0-00 kubeadm=1.24.0-00 kubectl=1.24.0-00
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl


# For Multimaster setup - execute the step to initalise the controller3 which is HA Proxy Controller
# Make sure Static DNS host enteries must be added in hosts files on all compute nodes
# Execute this command from controller0 with root or sudo privilege 

sudo kubeadm init --control-plane-endpoint controller3.13infotech.com:6443 --upload-certs

# if the step is succesful, you will get output like this

# Your Kubernetes control-plane has initialized successfully!

# To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

# Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

# Execute this step from controller1 and controller2 via #root or #sudo privilege
You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join controller3.13infotech.com:6443 --token xcqu59.994ldqbgjiwpxgv8 \
	--discovery-token-ca-cert-hash sha256:d2f9fe1b8e2b8fa9eff3090d7a9b9658f283a5446393d8bb72e280eca5cda1ab \
	--control-plane --certificate-key 8ab08b7b671dc1866b6494271f21e34cc79282186abea38bcac2f5bbb42dccaa

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.


# Execute this step from wn01, wn02 and wn03 via #root or #sudo privilege
Then you can join any number of worker nodes by running the following on each as root:

kubeadm join controller3.13infotech.com:6443 --token xcqu59.994ldqbgjiwpxgv8 \
	--discovery-token-ca-cert-hash sha256:d2f9fe1b8e2b8fa9eff3090d7a9b9658f283a5446393d8bb72e280eca5cda1ab
    

# For any reason, if kubeadm joining is failed. use this below command to reset the kubeadm activation steps process
kubeadm reset


# Add you newly given kubeadmin join codes like given in example above then you all control planes and worker nodes will join the cluster
# with Status NOT Ready


# Check the nodes status from master node using kubectl command
kubectl get nodes

# As we can see nodes status is ‘NotReady’, so to make it active.
# We must install CNI (Container Network Interface) or network add-on plugins like Calico, Flannel and Weave-net

curl https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml -O
# edit calico.yaml that are now downloaded on your controller and search CALICO_IP_V4POOL_CIDR
# unhash the values and replace the same CIDR that you have used while initializing the controller
- name: CALICO_IPV4POOL_CIDR
  value: '10.244.0.0/16'
# save the file and then enter the following command
kubectl apply -f calico.yaml

# Verify the status of pods in kube-system namespace
kubectl get pods -n kube-system

# Perfect, check the nodes status as well
kubectl get nodes

# Test Your Kubernetes Cluster Installation
kubectl create deployment nginx-app --image=nginx --replicas=2

# Expose the deployment as NodePort
kubectl expose deployment nginx-app --type=NodePort --port=80

# Run following commands to view service status
kubectl get svc nginx-app
kubectl describe svc nginx-app

# Use following curl command to access nginx based application
curl http://192.168.100.100:31246 # example only, please replace the static IP of your actual controller or node server

